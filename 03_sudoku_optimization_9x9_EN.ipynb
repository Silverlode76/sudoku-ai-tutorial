{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a076e760-af42-4932-8da4-344b9a00f52e",
   "metadata": {},
   "source": [
    "# Optimization for 9x9 Sudoku\n",
    "As disussed in Chapter 02 we will now extend our Sudoku - Puzzle from 4x4 to 9x9.\n",
    "We will check whether the puzzle is solved per:\n",
    "- `entropy_mean` which should converge to 0\n",
    "- `check_grid`which checks if the Sudoku Rules are fullfilled\n",
    "\n",
    "All needed functions are defined in `/source`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e6433e-c77e-4968-bd47-736ffb704da3",
   "metadata": {},
   "source": [
    "## 1.0 Initialization of 9x9 Grid\n",
    "- we have our puzzle which creates our Optimizer Tensor Z\n",
    "- Z will be hand over to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58684276-24dd-4f15-a836-6be7fe8eec80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import os, sys\n",
    "\n",
    "# In Colab: make sure you're in the repo folder\n",
    "if \"google.colab\" in sys.modules:\n",
    "    !git clone https://github.com/Silverlode76/sudoku-ai-tutorial.git\n",
    "    %cd sudoku-ai-tutorial\n",
    "\n",
    "# Ensure repo root is on Python path\n",
    "sys.path.insert(0, os.getcwd())\n",
    "\n",
    "print(\"cwd =\", os.getcwd())\n",
    "print(\"source exists =\", os.path.isdir(\"source\"))\n",
    "\n",
    "\n",
    "torch.set_printoptions(precision=4, sci_mode=False)\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "from source import probs_from_logits, pretty_grid_from_probs, optimize_sudoku, loss_dict, print_losses, check_grid\n",
    "from source import entropy_mean\n",
    "\n",
    "dim = 9\n",
    "grid = torch.tensor([\n",
    "    [5, 3, 0, 0, 7, 0, 0, 0, 0],\n",
    "    [6, 0, 0, 1, 9, 5, 0, 0, 0],\n",
    "    [0, 9, 8, 0, 0, 0, 0, 6, 0],\n",
    "\n",
    "    [8, 0, 0, 0, 6, 0, 0, 0, 3],\n",
    "    [4, 0, 0, 8, 0, 3, 0, 0, 1],\n",
    "    [7, 0, 0, 0, 2, 0, 0, 0, 6],\n",
    "\n",
    "    [0, 6, 0, 0, 0, 0, 2, 8, 0],\n",
    "    [0, 0, 0, 4, 1, 9, 0, 0, 5],\n",
    "    [0, 0, 0, 0, 8, 0, 0, 7, 9],\n",
    "], dtype=torch.long)\n",
    "\n",
    "# Build givens mask + targets (targets are digit-1 in [0..3])\n",
    "givens_mask = grid != 0\n",
    "givens_target = (grid-1).clamp(min=0)\n",
    "\n",
    "Z = torch.zeros((dim,dim,dim), dtype=torch.float32, device=device)\n",
    "\n",
    "# Strongly bias given cells (large positive logit for the correct digit, negative for others)\n",
    "high = 6.0\n",
    "low = -6.0\n",
    "for r in range(dim):\n",
    "    for c in range(dim):\n",
    "        if givens_mask[r,c]:\n",
    "            k = int(givens_target[r,c].item())  # 0..3\n",
    "            Z[r,c,:] = low\n",
    "            Z[r,c,k] = high\n",
    "\n",
    "\n",
    "grid, givens_mask, givens_target, Z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a529f09-a6b1-4cdc-841b-1dff5a3f7f6e",
   "metadata": {},
   "source": [
    "**Interpretation**  \n",
    "All needed input variables (Z, givens_mask and givens_target) for ADAM are defined "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f4a3a3-9b8c-4837-b1d9-9e52c357a0d6",
   "metadata": {},
   "source": [
    "## 2.0 Start Optimization \n",
    "Now we start the ADAM Optimizer with needed parameters as:\n",
    "- Iteration steps\n",
    "- T_start\n",
    "- T_end\n",
    "- lr (learning rate)\n",
    "- block_size 3\n",
    "\n",
    "At the end we will output some Graphs which shows that the loss function converges to 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f7f971-6bb1-4d7a-9e1b-71055949dc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "T_start = 1.2\n",
    "T_end = 0.7\n",
    "lr = 0.3\n",
    "block_size = 3\n",
    "\n",
    "Z_final, hist = optimize_sudoku(\n",
    "    Z, givens_mask, givens_target,\n",
    "    steps=100,\n",
    "    lr=lr,\n",
    "    T_start=T_start,\n",
    "    T_end=T_end,\n",
    "    block_size=block_size,\n",
    "    w_row=1.0,\n",
    "    w_col=1.0,\n",
    "    w_blk=1.2,\n",
    "    w_giv=2.0,\n",
    "    w_ent=0.01,\n",
    "    save_P_every=10,\n",
    ")\n",
    "\n",
    "P_final = probs_from_logits(Z_final, T=T_end)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(hist[\"L_total\"])\n",
    "plt.title(\"Total loss over iterations\")\n",
    "plt.xlabel(\"iteration\")\n",
    "plt.ylabel(\"L_total\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(hist[\"L_ent\"])\n",
    "plt.title(\"Entropy loss over iterations\")\n",
    "plt.xlabel(\"iteration\")\n",
    "plt.ylabel(\"L_ent\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(hist[\"T\"])\n",
    "plt.title(\"Temperature schedule\")\n",
    "plt.xlabel(\"iteration\")\n",
    "plt.ylabel(\"T\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0bb342-c68d-41e3-9df4-f859ddaa1c14",
   "metadata": {},
   "source": [
    "**Interpretation**  \n",
    "We observe that the total_loss diverges to 0 at step = 20.\n",
    "But we do not know if the solution at step = 20 is correct or not.\n",
    "\n",
    "Let's take a closer look to this by checking every 10th step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ad9795-07ee-4578-85bd-c5e01c4b8161",
   "metadata": {},
   "outputs": [],
   "source": [
    "for step, P in hist[\"P_snapshots\"].items():\n",
    "    mean_entropy = entropy_mean(P)\n",
    "    #ent = -(P * (P + 1e-9).log()).sum(dim=2)   # (9,9)\n",
    "    print(step, \"mean_entropy=\", mean_entropy)\n",
    "    check_grid(P)\n",
    "    conf = P.max(dim=2).values                 # (9,9)\n",
    "    print(step, \"mean_maxP=\", conf.mean().item(), \"min_maxP=\", conf.min().item())\n",
    "    print(\"---------------------------------------\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17519ed9-b029-4750-9d51-8d898b7f9036",
   "metadata": {},
   "source": [
    "## Inspecting the optimization trajectory (`P_snapshots`)\n",
    "\n",
    "During optimization we store intermediate probability tensors `P` in `hist[\"P_snapshots\"]`.\n",
    "Each snapshot represents the model’s current belief distribution over digits for every cell.\n",
    "\n",
    "The loop below prints a compact set of diagnostics for each stored step:\n",
    "\n",
    "- **Mean entropy (`mean_entropy`)**  \n",
    "  Entropy measures uncertainty.  \n",
    "  - high entropy → probabilities are spread out (the model is unsure)  \n",
    "  - low entropy → probabilities are peaked (the model is confident)\n",
    "\n",
    "  Tracking the *mean* entropy over all 81 cells tells us whether the solution is becoming more “decisive” over time.\n",
    "\n",
    "- **Constraint check (`check_grid(P)`)**  \n",
    "  This is a sanity check: we convert the current probabilities to a discrete grid (typically via `argmax`) and verify whether row/column/block rules are satisfied (or how badly they are violated).  \n",
    "  Even when the final solution is not perfect yet, the violations should generally decrease across steps.\n",
    "\n",
    "- **Confidence statistics (`mean_maxP` and `min_maxP`)**  \n",
    "  `P.max(dim=2).values` yields, for each cell, the probability of the currently most likely digit.  \n",
    "  - `mean_maxP` → average confidence across the grid  \n",
    "  - `min_maxP` → the “weakest” cell (least confident argmax)\n",
    "\n",
    "  This is useful because a Sudoku often fails due to a small number of ambiguous cells:\n",
    "  even if most cells become confident, one or two low-confidence cells can still break a row/col/block.\n",
    "\n",
    "Overall, these diagnostics help us separate two questions:\n",
    "1) Are we becoming **more confident**? (entropy ↓, maxP ↑)  \n",
    "2) Are we becoming **more correct** w.r.t. Sudoku constraints? (check_grid improves)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421747f2-f070-4da6-aa4d-f960f791b0f3",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook we demonstrated how a Sudoku can be formulated as a **differentiable constraint satisfaction problem**.\n",
    "\n",
    "Key takeaways:\n",
    "\n",
    "- A Sudoku grid `(r × c)` can be extended into a probability tensor `P (r × c × k)`, where each cell represents a distribution over possible digits.\n",
    "- Sudoku rules (row, column, and block uniqueness) can be expressed as **soft constraints** and combined into a loss function.\n",
    "- Given digits are enforced via a mask, anchoring the optimization to the known clues.\n",
    "- A single optimization step already moves the probability mass toward a valid solution.\n",
    "- No search, backtracking, or discrete solver is required — the structure emerges from the constraints.\n",
    "\n",
    "This approach highlights an important idea:\n",
    "**Sudoku solving can be seen as continuous optimization guided by structure, rather than discrete trial-and-error.**\n",
    "\n",
    "In the next steps, we will:\n",
    "- iterate the optimization until convergence,\n",
    "- analyze failure cases,\n",
    "- and discuss practical stabilization techniques.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ed10db-9ad7-4c7f-aad5-4f721365f900",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
