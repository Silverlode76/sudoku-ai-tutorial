{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39dd7e91",
   "metadata": {},
   "source": [
    "# 02 — Optimization with PyTorch (4×4 Sudoku)\n",
    "\n",
    "This notebook builds on **Chapter 01** (tensor basics + differentiable constraints).  \n",
    "Here we focus on **how** an optimizer turns those constraints into an actual Sudoku solution.\n",
    "\n",
    "> Key idea: we do **not** train a neural network.  \n",
    "> We directly optimize a **tensor of logits** whose softmax becomes the probability tensor **P**.\n",
    "\n",
    "The 4×4 case primarily serves as a didactic example to illustrate\n",
    "the continuous relaxation and optimization dynamics.\n",
    "Due to the small problem size, convergence is typically very fast,\n",
    "even for puzzles with many unknown cells.\n",
    "\n",
    "\n",
    "---\n",
    "## 2.1 Recap: The Constraint Landscape\n",
    "A valid Sudoku solution is defined by constraints:\n",
    "- **Row uniqueness**: each digit appears exactly once per row\n",
    "- **Column uniqueness**: each digit appears exactly once per column\n",
    "- **Block uniqueness**: each digit appears exactly once per block\n",
    "- **Givens**: given cells must keep the correct digit\n",
    "- Optional: **entropy** to encourage confident (nearly one-hot) predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06bef76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.set_printoptions(precision=4, sci_mode=False)\n",
    "device = torch.device(\"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4384a355",
   "metadata": {},
   "source": [
    "---\n",
    "## 2.2 What Does It Mean to Optimize **P**?\n",
    "We represent a 4×4 Sudoku as a tensor of probabilities:\n",
    "\n",
    "- **P** has shape **(4, 4, 4)**\n",
    "- `P[r, c, k]` is the probability that cell `(r, c)` contains digit `k+1`.\n",
    "\n",
    "During optimization, we do **not** update `P` directly.  \n",
    "Instead we optimize **logits** `Z` and compute:\n",
    "\n",
    "\\[ P = \\text{softmax}(Z / T) \\]\n",
    "\n",
    "where `T` is a temperature (optionally annealed over time).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44520685",
   "metadata": {},
   "source": [
    "---\n",
    "## 2.3 Parameterization: Logits → Softmax → Probabilities\n",
    "\n",
    "Why optimize logits?\n",
    "- logits are unconstrained real numbers\n",
    "- softmax turns them into a valid probability distribution per cell\n",
    "- probabilities stay well-behaved during gradient updates\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e041ded-2442-4c2d-955f-e77ea9371e5f",
   "metadata": {},
   "source": [
    "## Help Functions\n",
    "- `probs_from_logits` applies a softmax over the digit dimension `k`,\n",
    "  ensuring that the probabilities for each cell sum to 1.\n",
    "- `argmax` selects the most likely digit per cell and is used only for\n",
    "  visualization, not during optimization. argmax returns the first occurrence of the maximum value.\n",
    "- `loss-dict` calls the loss function `sudoku_losses`and prints the calclated losses for P "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d2c471",
   "metadata": {},
   "outputs": [],
   "source": [
    "def probs_from_logits(Z, T=1.0):\n",
    "    # Z: (4,4,4) logits\n",
    "    # returns P: (4,4,4) probs\n",
    "    return F.softmax(Z / T, dim=2)\n",
    "\n",
    "def pretty_grid_from_probs(P):\n",
    "    # takes argmax per cell, returns digits 1..4\n",
    "    return P.argmax(dim=2) + 1\n",
    "\n",
    "def loss_dict(P, givens_mask, givens_target, weights):\n",
    "    w_row, w_col, w_blk, w_giv, w_ent = weights\n",
    "    L_row, L_col, L_blk, L_giv, L_ent = sudoku_losses(P, givens_mask, givens_target)\n",
    "    L_total = w_row*L_row + w_col*L_col + w_blk*L_blk + w_giv*L_giv + w_ent*L_ent\n",
    "    return {\n",
    "        \"L_row\": L_row,\n",
    "        \"L_col\": L_col,\n",
    "        \"L_blk\": L_blk,\n",
    "        \"L_giv\": L_giv,\n",
    "        \"L_ent\": L_ent,\n",
    "        \"L_total\": L_total,\n",
    "    }\n",
    "\n",
    "def print_losses(tag, d):\n",
    "    # .item() macht aus tensor(...) einen float, ohne grad_fn-Anzeige\n",
    "    print(\n",
    "    tag,\n",
    "    {k: f\"{v.detach().cpu().item():.3f}\" for k, v in d.items()},\n",
    "    \"\\n\"\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbca4e2d",
   "metadata": {},
   "source": [
    "---\n",
    "## 2.4 The Optimizer (Adam) — High-Level View\n",
    "\n",
    "Each iteration does:\n",
    "1. logits `Z` → probabilities `P`\n",
    "2. compute constraint losses\n",
    "3. sum into `L_total`\n",
    "4. backpropagate gradients w.r.t. `Z`\n",
    "5. `optimizer.step()` updates `Z`\n",
    "\n",
    "Next, we implement the constraint losses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8687e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sudoku_losses(P, givens_mask, givens_target, eps=1e-8):\n",
    "    \"\"\"\n",
    "    P: (4,4,4) probabilities over digits 1..4 (k dimension last)\n",
    "    givens_mask: (4,4) bool, True where a clue is given\n",
    "    givens_target: (4,4) long, values in [0..3] for digit indices (digit-1)\n",
    "    \"\"\"\n",
    "    # Row uniqueness: for each row i and digit k, sum over columns j should be 1\n",
    "    row_sum = P.sum(dim=1)                 # (4,4)\n",
    "    L_row = ((row_sum - 1.0) ** 2).sum()\n",
    "\n",
    "    # Column uniqueness: for each col j and digit k, sum over rows i should be 1\n",
    "    col_sum = P.sum(dim=0)                 # (4,4)\n",
    "    L_col = ((col_sum - 1.0) ** 2).sum()\n",
    "\n",
    "    # Block uniqueness: 2×2 blocks\n",
    "    L_blk = 0.0\n",
    "    for br in (0, 2):\n",
    "        for bc in (0, 2):\n",
    "            blk = P[br:br+2, bc:bc+2, :]   # (2,2,4)\n",
    "            blk_sum = blk.sum(dim=(0,1))   # (4,)\n",
    "            L_blk = L_blk + ((blk_sum - 1.0) ** 2).sum()\n",
    "\n",
    "    # Givens loss: encourage given cells to keep probability 1 at the given digit\n",
    "    # pick probabilities at given positions\n",
    "    given_probs = P[givens_mask]                       # (num_givens, 4)\n",
    "    given_targets = givens_target[givens_mask]         # (num_givens,)\n",
    "    # negative log-likelihood (cross entropy) on givens\n",
    "    L_giv = F.nll_loss((given_probs + eps).log(), given_targets, reduction='sum')\n",
    "\n",
    "    # Entropy: encourage low entropy (more confident predictions)\n",
    "    ent = -(P * (P + eps).log()).sum(dim=2)            # (4,4)\n",
    "    L_ent = ent.sum()\n",
    "\n",
    "    return L_row, L_col, L_blk, L_giv, L_ent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99767546",
   "metadata": {},
   "source": [
    "---\n",
    "## 2.5 One Optimization Step (Concrete!)\n",
    "We demonstrate a single optimizer step on an **almost solved** 4×4 Sudoku.\n",
    "\n",
    "Sudoku Puzzle:\n",
    "```\n",
    "[1, 2, 3, 4]\n",
    "[3, 4, 1, 2]\n",
    "[2, 1, 4, 3]\n",
    "[4, 3, 0, 1]\n",
    "```\n",
    "\n",
    "Only one cell [3,2| is unknown. This makes it easy to see how the optimizer shifts\n",
    "probabilities away from inconsistent digits and towards the only consistent digit.\n",
    "\n",
    "If we translate the Sudoku Puzzle to `P`, we get the following cube:\n",
    "\n",
    "<img src=\"images/02_P_Cube_4x4x4.jpg\" alt=\"4×4×4 Sudoku as a probability tensor cube\" width=\"200\"/>\n",
    "\n",
    "- Gray cubes represent a probability of `0.25` (uniform uncertainty)\n",
    "- Beige cubes represent `0.0`\n",
    "- Black cubes represent given digits with value `1.0`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee50536b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Almost solved 4×4 Sudoku (0 = unknown)\n",
    "# Digits are 1..4\n",
    "grid = torch.tensor([\n",
    "    [1, 2, 3, 4],\n",
    "    [3, 4, 1, 2],\n",
    "    [2, 1, 4, 3],\n",
    "    [4, 3, 0, 1],\n",
    "], dtype=torch.long)\n",
    "\n",
    "# Build givens mask + targets (targets are digit-1 in [0..3])\n",
    "givens_mask = grid != 0\n",
    "givens_target = (grid-1).clamp(min=0)\n",
    "\n",
    "grid, givens_mask, givens_target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11fd5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize logits Z.\n",
    "# For given cells, we initialize logits such that the correct digit is strongly preferred.\n",
    "# For the unknown cell, logits start at 0 -> uniform probabilities after softmax.\n",
    "\n",
    "Z = torch.zeros((4,4,4), dtype=torch.float32, device=device)\n",
    "\n",
    "# Strongly bias given cells (large positive logit for the correct digit, negative for others)\n",
    "high = 6.0\n",
    "low = -6.0\n",
    "for r in range(4):\n",
    "    for c in range(4):\n",
    "        if givens_mask[r,c]:\n",
    "            k = int(givens_target[r,c].item())  # 0..3\n",
    "            Z[r,c,:] = low\n",
    "            Z[r,c,k] = high\n",
    "\n",
    "# Check probabilities at the unknown cell (3,2)\n",
    "P0 = probs_from_logits(Z, T=1.0)\n",
    "Z, P0, pretty_grid_from_probs(P0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f7cb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute losses BEFORE the step\n",
    "L_row, L_col, L_blk, L_giv, L_ent = sudoku_losses(P0, givens_mask, givens_target)\n",
    "L_row, L_col, L_blk, L_giv, L_ent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc18330f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One optimizer step\n",
    "weights = (1.0, 1.0, 1.2, 2.0, 0.01)\n",
    "\n",
    "Z_step = Z.clone().detach().requires_grad_(True)\n",
    "opt = torch.optim.Adam([Z_step], lr=0.3)\n",
    "\n",
    "# BEFORE step\n",
    "P = probs_from_logits(Z_step, T=1.0)\n",
    "print (\"P-before\", P,\"\\n\", pretty_grid_from_probs(P), \"\\n\")\n",
    "d0 = loss_dict(P, givens_mask, givens_target, weights)\n",
    "print_losses(\"P  (before)\", d0)\n",
    "\n",
    "# update\n",
    "opt.zero_grad()\n",
    "d0[\"L_total\"].backward()\n",
    "opt.step()\n",
    "\n",
    "# AFTER step\n",
    "P1 = probs_from_logits(Z_step, T=1.0)  # detach nicht nötig fürs Anzeigen\n",
    "d1 = loss_dict(P1, givens_mask, givens_target, weights)\n",
    "print (\"P1-after\", P1,\"\\n\", pretty_grid_from_probs(P1), \"\\n\")\n",
    "print_losses(\"P1 (after) \", d1)\n",
    "\n",
    "# Deltas\n",
    "print(\"Δ (after - before):\",\n",
    "      {k: float((d1[k]-d0[k]).detach().cpu().item()) for k in d0.keys()})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b255c404-bfdb-4aee-8952-0db5d63066dc",
   "metadata": {},
   "source": [
    "### 2.6 For Loop\n",
    "Let's do a for Loop to show the convergencey more cleraly. \n",
    "You will oberserve that cell P(3,2) converges quickly from \n",
    "\n",
    "`[0.25,0.25,0.25]`\n",
    "to\n",
    "`[0.0080, 0.9761, 0.0080, 0.0080]`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3cc8a3-6dd4-4de8-8324-4d25bd2ae2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = (1.0, 1.0, 1.2, 2.0, 0.01)\n",
    "\n",
    "Z_step = Z.clone().detach().requires_grad_(True)\n",
    "opt = torch.optim.Adam([Z_step], lr=0.3)\n",
    "\n",
    "unknown_rc = (3, 2)\n",
    "\n",
    "# Baseline (Iteration 0)\n",
    "P0 = probs_from_logits(Z_step, T=1.0)\n",
    "d_base = loss_dict(P0, givens_mask, givens_target, weights)\n",
    "\n",
    "print(\"Iter 0\")\n",
    "print(\"Grid:\\n\", pretty_grid_from_probs(P0))\n",
    "print(\"Unknown cell probs:\", P0[unknown_rc[0], unknown_rc[1], :].detach().cpu())\n",
    "print_losses(\"Losses\", d_base)\n",
    "print()\n",
    "\n",
    "# Loop\n",
    "steps = 10\n",
    "print_every = 1  # z.B. 1 = jede Iteration, 5 = jede 5te\n",
    "\n",
    "for it in range(1, steps + 1):\n",
    "    P = probs_from_logits(Z_step, T=1.0)\n",
    "    d = loss_dict(P, givens_mask, givens_target, weights)\n",
    "\n",
    "    opt.zero_grad()\n",
    "    d[\"L_total\"].backward()\n",
    "    opt.step()\n",
    "\n",
    "    # Anzeige nach dem Update\n",
    "    P_after = probs_from_logits(Z_step, T=1.0)\n",
    "    d_after = loss_dict(P_after, givens_mask, givens_target, weights)\n",
    "\n",
    "    if it % print_every == 0:\n",
    "        print(f\"Iter {it}\")\n",
    "        print(\"Grid:\\n\", pretty_grid_from_probs(P_after))\n",
    "        print(\"Unknown cell probs:\", P_after[unknown_rc[0], unknown_rc[1], :].detach().cpu())\n",
    "\n",
    "        # hübsch: before/after/delta zur Baseline\n",
    "        deltas = {k: float((d_after[k] - d_base[k]).detach().cpu().item()) for k in d_base.keys()}\n",
    "        print_losses(\"Losses\", d_after)\n",
    "        print(\"Δ vs Iter0:\", {k: f\"{v:+.3f}\" for k, v in deltas.items()})\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42ce527",
   "metadata": {},
   "source": [
    "**Interpretation**  \n",
    "The optimizer does not “know” Sudoku. It follows gradients created by the constraints.  \n",
    "Digits that violate row/column/block consistency are pushed down, while the consistent digit is pushed up.\n",
    "\n",
    "In a less constrained puzzle, this process takes many iterations and benefits from entropy + temperature control.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e593cd1f",
   "metadata": {},
   "source": [
    "---\n",
    "## 2.6 Entropy and Temperature Annealing\n",
    "\n",
    "- **Entropy loss** encourages probabilities to become confident (close to one-hot).\n",
    "- **Temperature** controls how sharp softmax outputs are:\n",
    "  - high `T` → softer probabilities (more exploration)\n",
    "  - low `T` → sharper probabilities (more exploitation)\n",
    "\n",
    "A common strategy is to start with a higher temperature and slowly decrease it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dffdf46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_sudoku(Z_init, givens_mask, givens_target, steps=300, lr=0.2,\n",
    "                   T_start=1.5, T_end=0.6, w_row=1.0, w_col=1.0, w_blk=1.2, w_giv=2.0, w_ent=0.01,\n",
    "                   save_P_every=5):\n",
    "    Z = Z_init.clone().detach().requires_grad_(True)\n",
    "    opt = torch.optim.Adam([Z], lr=lr)\n",
    "\n",
    "    hist = {\n",
    "        \"L_total\": [], \"L_row\": [], \"L_col\": [], \"L_blk\": [], \"L_giv\": [], \"L_ent\": [], \"T\": [], \n",
    "        \"P_snapshots\": {}   # step -> P\n",
    "    }\n",
    "\n",
    "    for t in range(steps):\n",
    "        # linear temperature schedule\n",
    "        alpha = t / max(1, steps-1)\n",
    "        T = (1-alpha)*T_start + alpha*T_end\n",
    "\n",
    "        P = probs_from_logits(Z, T=T)\n",
    "        L_row, L_col, L_blk, L_giv, L_ent = sudoku_losses(P, givens_mask, givens_target)\n",
    "        L_total = w_row*L_row + w_col*L_col + w_blk*L_blk + w_giv*L_giv + w_ent*L_ent\n",
    "\n",
    "        opt.zero_grad()\n",
    "        L_total.backward()\n",
    "        opt.step()\n",
    "\n",
    "        hist[\"L_total\"].append(float(L_total.detach().cpu()))\n",
    "        hist[\"L_row\"].append(float(L_row.detach().cpu()))\n",
    "        hist[\"L_col\"].append(float(L_col.detach().cpu()))\n",
    "        hist[\"L_blk\"].append(float(L_blk.detach().cpu()))\n",
    "        hist[\"L_giv\"].append(float(L_giv.detach().cpu()))\n",
    "        hist[\"L_ent\"].append(float(L_ent.detach().cpu()))\n",
    "        hist[\"T\"].append(float(T))\n",
    "        # store intermediate P\n",
    "        if t % save_P_every == 0 or t == steps - 1:\n",
    "            hist[\"P_snapshots\"][t] = P.detach().cpu()\n",
    "\n",
    "    return Z.detach(), hist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46f40ab-25c4-4494-9031-a6a5032f8a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the full optimization on the same \"almost solved\" puzzle (converges quickly)\n",
    "Z_final, hist = optimize_sudoku(Z, givens_mask, givens_target, steps=10, lr=0.3, T_start=1.2, T_end=0.7)\n",
    "P_final = probs_from_logits(Z_final, T=0.7)\n",
    "pretty_grid_from_probs(P_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21017999",
   "metadata": {},
   "source": [
    "---\n",
    "## 2.7 Convergence Behavior and Failure Modes\n",
    "\n",
    "Typical issues when scaling to harder puzzles:\n",
    "- **Local minima**: the optimizer gets stuck in a partially consistent state\n",
    "- **Learning rate too high**: oscillations or instability\n",
    "- **Weights poorly tuned**: one constraint dominates and blocks progress\n",
    "- **Entropy too strong too early**: forces premature, wrong decisions\n",
    "\n",
    "Below we visualize convergence (loss over iterations) for this simple example.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9825a8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(hist[\"L_total\"])\n",
    "plt.title(\"Total loss over iterations\")\n",
    "plt.xlabel(\"iteration\")\n",
    "plt.ylabel(\"L_total\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(hist[\"L_ent\"])\n",
    "plt.title(\"Entropy loss over iterations\")\n",
    "plt.xlabel(\"iteration\")\n",
    "plt.ylabel(\"L_ent\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(hist[\"T\"])\n",
    "plt.title(\"Temperature schedule\")\n",
    "plt.xlabel(\"iteration\")\n",
    "plt.ylabel(\"T\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070ebae3-a896-4a62-9084-b9af8101d4a4",
   "metadata": {},
   "source": [
    "### 2.7.1 Observations \n",
    "After 4 steps we have already a Total_Loss of almost 0 which shows that the solution is found very quickly. \n",
    "\n",
    "In this context, a solution is considered found once the `argmax`\n",
    "projection yields a grid that satisfies all Sudoku constraints.\n",
    "\n",
    "\n",
    "Let's try with more unknowns and analyze the results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97c942e-542a-43d6-b858-acdd1ae73a51",
   "metadata": {},
   "source": [
    "### 2.7.2 More Unkowns\n",
    "In this example we will select more unknowns in order to show how the optimizer works for a mutlidimensional field.\n",
    "We select as Puzzle following grid\n",
    "```\n",
    "[0, 0, 3, 1]\n",
    "[1, 0, 0, 0]\n",
    "[0, 2, 1, 0]\n",
    "[0, 0, 4, 0]\n",
    "```\n",
    "So our unknowns are P(0,0), P(0,1), P(1,1), P(1,2), P(1,3), P(2,0), P(2,3), P(3,0), P(3,1) and P(3,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0ddd4c-4747-4d55-a9fe-d0c25bb3d59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Almost solved 4×4 Sudoku (0 = unknown)\n",
    "# Digits are 1..4\n",
    "grid = torch.tensor([\n",
    "    [0, 0, 3, 1],\n",
    "    [1, 0, 0, 0],\n",
    "    [0, 2, 1, 3],\n",
    "    [0, 0, 4, 0],\n",
    "], dtype=torch.long)\n",
    "\n",
    "# Build givens mask + targets (targets are digit-1 in [0..3])\n",
    "givens_mask = grid != 0\n",
    "givens_target = (grid-1).clamp(min=0)\n",
    "\n",
    "grid, givens_mask, givens_target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562f9d18-f604-45cf-bfc1-2cb7525987e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize logits Z.\n",
    "# For given cells, we initialize logits such that the correct digit is strongly preferred.\n",
    "# For the unknown cell, logits start at 0 -> uniform probabilities after softmax.\n",
    "\n",
    "Z = torch.zeros((4,4,4), dtype=torch.float32, device=device)\n",
    "\n",
    "# Strongly bias given cells (large positive logit for the correct digit, negative for others)\n",
    "high = 6.0\n",
    "low = -6.0\n",
    "for r in range(4):\n",
    "    for c in range(4):\n",
    "        if givens_mask[r,c]:\n",
    "            k = int(givens_target[r,c].item())  # 0..3\n",
    "            Z[r,c,:] = low\n",
    "            Z[r,c,k] = high\n",
    "\n",
    "# Check probabilities at the unknown cell (3,2)\n",
    "P0 = probs_from_logits(Z, T=1.0)\n",
    "Z, P0, pretty_grid_from_probs(P0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8698da-bb0b-4a54-82ef-a78274f3e478",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the full optimization on the same \"almost solved\" puzzle (converges quickly)\n",
    "Z_final, hist = optimize_sudoku(Z, givens_mask, givens_target, steps=10, lr=0.3, T_start=1.2, T_end=0.7)\n",
    "P_final = probs_from_logits(Z_final, T=0.7)\n",
    "print(\"Final solution:\\n\",pretty_grid_from_probs(P_final))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55a4ff3-e8a3-4f00-b037-8825505db3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(hist[\"L_total\"])\n",
    "plt.title(\"Total loss over iterations\")\n",
    "plt.xlabel(\"iteration\")\n",
    "plt.ylabel(\"L_total\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(hist[\"L_ent\"])\n",
    "plt.title(\"Entropy loss over iterations\")\n",
    "plt.xlabel(\"iteration\")\n",
    "plt.ylabel(\"L_ent\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(hist[\"T\"])\n",
    "plt.title(\"Temperature schedule\")\n",
    "plt.xlabel(\"iteration\")\n",
    "plt.ylabel(\"T\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709d2207-d4ac-48c8-bedb-cb8d0b16cb53",
   "metadata": {},
   "source": [
    "### 2.7.3 Observations (More Unknowns)\n",
    "\n",
    "With more unknown cells, the optimization starts from a much larger feasible space.\n",
    "As a result, the total loss typically decreases **less steeply** in the first iterations\n",
    "compared to the “almost solved” case.\n",
    "\n",
    "However, for 4×4 Sudoku the constraint landscape is still very small and strongly structured.\n",
    "In this example, the optimizer reaches a discrete-looking solution quickly:\n",
    "after about 10 iterations, the `argmax` projection already yields a valid Sudoku grid.\n",
    "\n",
    "Note that we optimize a **continuous relaxation** `P`, while the displayed grid is obtained via `argmax`.\n",
    "Therefore, “finding the solution” here means: *the `argmax` projection becomes consistent with all constraints*,\n",
    "even though the underlying probabilities may still continue to sharpen slightly afterwards.\n",
    "\n",
    "In practice, 4×4 Sudoku often converges very fast under this relaxation,\n",
    "because the problem is small (only 16 cells) and the constraints (rows/cols/blocks) are very strong.\n",
    "More interesting convergence behavior typically appears when scaling to 9×9.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1fd6cd-1eec-46c4-a582-9ad13df1111e",
   "metadata": {},
   "source": [
    "### 2.7.4 Limitations of the 4×4 Case\n",
    "\n",
    "- The optimization landscape is very small and strongly constrained.\n",
    "- Differences in puzzle difficulty are compressed into very few iterations.\n",
    "- Loss curves alone are not sufficient to characterize convergence behavior.\n",
    "\n",
    "These limitations motivate the extension to 9×9 Sudoku.\n",
    "\n",
    "In the next chapter, we extend the same formulation to 9×9 Sudoku.\n",
    "At this scale, the optimization dynamics become significantly more interesting,\n",
    "with longer plateaus, stronger symmetry effects, and slower convergence.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc87dc6",
   "metadata": {},
   "source": [
    "---\n",
    "## Next\n",
    "In Chapter 03 we scale the same ideas to **9×9 Sudoku**:\n",
    "- tensor becomes `(9, 9, 9)`\n",
    "- blocks become `3×3`\n",
    "- optimization becomes harder and needs careful tuning\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
